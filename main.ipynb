{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import json, os\n",
    "from copy import deepcopy\n",
    "from pdf2image import convert_from_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[[1, 4], [3, 4], [3, 2], [1, 2]]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PDF_PATH = './pdfs'\n",
    "IMAGE_PATH = './images'\n",
    "def save_image(pdf_name):\n",
    "    file_path = os.path.join(PDF_PATH, f'{pdf_name}.pdf')\n",
    "    img = convert_from_path(file_path)[0]\n",
    "    width, height = img.size\n",
    "    fname = f'{pdf_name}.png'\n",
    "    img.save(os.path.join(IMAGE_PATH, fname))\n",
    "    return {\"width\" : width, \"height\" : height, \"fname\" : fname}\n",
    "\n",
    "def get_box(position):\n",
    "    x1, y1, x2, y2 = position[:]\n",
    "    return [[x1, y2], [x2, y2], [x2, y1], [x1, y1]]\n",
    "\n",
    "get_box([1,2,3,4])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def add_query(record_with_query, query):\n",
    "    question_id = str(query[\"metadata\"][\"question_id\"])\n",
    "    record_with_query[\"id\"] += '_' + question_id\n",
    "    record_with_query[\"input\"][\"id\"] += '_' + question_id\n",
    "    record_with_query[\"input\"][\"uid\"] += '_' + question_id\n",
    "    record_with_query[\"query\"] = query[\"key\"]\n",
    "    record_with_query[\"instruction\"] = query[\"key\"]\n",
    "    record_with_query[\"output\"] = []\n",
    "    for value in query[\"values\"]:\n",
    "        record_with_query[\"output\"].extend(value[\"value_variants\"])\n",
    "    return record_with_query\n",
    "\n",
    "def add_ocr(record_with_doc, content):\n",
    "    tool_name = content[\"tool_name\"]\n",
    "    if tool_name == \"tesseract\":\n",
    "        tokens_layer = content[\"tokens_layer\"]\n",
    "    elif tool_name == \"microsoft_cv\":\n",
    "        tokens_layer = content[\"common_format\"]\n",
    "    else:\n",
    "        raise ValueError(\"不支持的OCR格式\")\n",
    "    record_with_doc[\"id\"] += '_' + tool_name[0:2]\n",
    "    record_with_doc[\"input\"][\"id\"] += '_' + tool_name[0:2]\n",
    "    record_with_doc[\"input\"][\"uid\"] += '_' + tool_name[0:2]\n",
    "\n",
    "    tokens = tokens_layer[\"tokens\"]\n",
    "    token_positions = tokens_layer[\"positions\"]\n",
    "    lines = tokens_layer[\"structures\"][\"lines\"]\n",
    "    structure_value = lines[\"structure_value\"]\n",
    "    seg_positions = lines[\"positions\"]\n",
    "    line_num = len(lines[\"structure_value\"])\n",
    "\n",
    "    document = []\n",
    "    for line_idx in range(line_num):\n",
    "        cur_line_dict = {\"id\": line_idx, \"box\": get_box(seg_positions[line_idx])}\n",
    "\n",
    "        line_start_idx = structure_value[line_idx][0]\n",
    "        line_end_idx = structure_value[line_idx][1]\n",
    "        cur_line_dict[\"text\"] = \" \".join(tokens[line_start_idx:line_end_idx])\n",
    "\n",
    "        cur_line_dict_words = []\n",
    "        for token_idx in range(line_start_idx, line_end_idx):\n",
    "            cur_line_dict_words_dict = {\"id\": token_idx,\n",
    "                                        \"box\": get_box(token_positions[token_idx]),\n",
    "                                        \"text\": tokens[token_idx]}\n",
    "            cur_line_dict_words.append(cur_line_dict_words_dict)\n",
    "\n",
    "        cur_line_dict[\"words\"] = cur_line_dict_words\n",
    "        document.append(cur_line_dict)\n",
    "\n",
    "    record_with_doc[\"input\"][\"document\"] = document\n",
    "    return record_with_doc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "JSON_PATH = \"./jsons\"\n",
    "INDEX_PATH = \"./\"\n",
    "def process_vqa_line(dataset_name, split, line1, line2):\n",
    "    record_query = json.loads(line1)\n",
    "    record_ocr = json.loads(line2)\n",
    "    pdf_name = record_ocr[\"name\"]\n",
    "    split_name = record_query[\"split\"]\n",
    "    full_id = dataset_name + '_' + split + '_' + pdf_name\n",
    "    record_init = {\"id\": full_id,\n",
    "                  \"input\": {\"id\" : full_id, \"uid\" : full_id,\n",
    "                            \"image\" : save_image(pdf_name)}}\n",
    "    query_num = len(record_query['annotations'])\n",
    "    ocr_num = len(record_ocr[\"contents\"])\n",
    "    for ocr_id in range(ocr_num):\n",
    "        record_with_doc = deepcopy(record_init)\n",
    "        record_with_doc = add_ocr(record_with_doc, record_ocr[\"contents\"][ocr_id])\n",
    "        for query_id in range(query_num):\n",
    "            record_final = deepcopy(record_with_doc)\n",
    "            record_final = add_query(record_final, record_query['annotations'][query_id])\n",
    "            write_json(record_final)\n",
    "            write_index_file(dataset_name, pdf_name, record_final[\"id\"], split_name)\n",
    "\n",
    "\n",
    "# 逐行读取 JSONL 文件并处理记录\n",
    "def process_vqa_jsonl(dataset_name, split, batch_size = None):\n",
    "    file1_path = os.path.join(os.path.abspath(\"\"), dataset_name, split, 'document.jsonl')\n",
    "    file2_path = os.path.join(os.path.abspath(\"\"), dataset_name, split, 'documents_content.jsonl')\n",
    "\n",
    "    with open(file1_path, 'r', encoding=\"utf-8\") as file1, open(file2_path, 'r', encoding=\"utf-8\") as file2:\n",
    "        for i, (line1, line2) in enumerate(zip(file1, file2)):\n",
    "            if i == batch_size:\n",
    "                break\n",
    "            process_vqa_line(dataset_name, split, line1, line2)\n",
    "\n",
    "# 将处理后的记录写入 JSON 文件\n",
    "def write_json(processed_record):\n",
    "    file_name = processed_record[\"id\"]\n",
    "    with open(os.path.join(JSON_PATH, file_name), 'w') as json_file:\n",
    "        json.dump(processed_record, json_file, indent=4)\n",
    "\n",
    "def write_index_file(dataset_name, image_file_name, json_file_name, split_name):\n",
    "    image_file_name += '.png'\n",
    "    json_file_name += '.json'\n",
    "    index_file_name = dataset_name + '.' + split_name + '.txt'\n",
    "    with open(index_file_name, 'w') as index_file:\n",
    "        index_file.write(image_file_name + '\\t' + json_file_name + '\\n')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "process_vqa_jsonl(\"docvqa\", \"\", 3)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "huggingface",
   "language": "python",
   "display_name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
